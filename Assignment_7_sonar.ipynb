{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 7 sonar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj_e8JXeTzhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d237ea9d-ccb7-4d11-ff1a-82cfbdf42f06"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import seaborn as sb\n",
        "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5gvE4iZUE_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "2ca39a6c-2cdd-4cbf-c3af-26f26aa0f695"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Bootcamp ML2020/')\n",
        "os.listdir('.')\n",
        "dataset = pd.read_csv('sonar.csv')\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>V29</th>\n",
              "      <th>V30</th>\n",
              "      <th>V31</th>\n",
              "      <th>V32</th>\n",
              "      <th>V33</th>\n",
              "      <th>V34</th>\n",
              "      <th>V35</th>\n",
              "      <th>V36</th>\n",
              "      <th>V37</th>\n",
              "      <th>V38</th>\n",
              "      <th>V39</th>\n",
              "      <th>V40</th>\n",
              "      <th>V41</th>\n",
              "      <th>V42</th>\n",
              "      <th>V43</th>\n",
              "      <th>V44</th>\n",
              "      <th>V45</th>\n",
              "      <th>V46</th>\n",
              "      <th>V47</th>\n",
              "      <th>V48</th>\n",
              "      <th>V49</th>\n",
              "      <th>V50</th>\n",
              "      <th>V51</th>\n",
              "      <th>V52</th>\n",
              "      <th>V53</th>\n",
              "      <th>V54</th>\n",
              "      <th>V55</th>\n",
              "      <th>V56</th>\n",
              "      <th>V57</th>\n",
              "      <th>V58</th>\n",
              "      <th>V59</th>\n",
              "      <th>V60</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.1609</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>0.2273</td>\n",
              "      <td>0.3100</td>\n",
              "      <td>0.2999</td>\n",
              "      <td>0.5078</td>\n",
              "      <td>0.4797</td>\n",
              "      <td>0.5783</td>\n",
              "      <td>0.5071</td>\n",
              "      <td>0.4328</td>\n",
              "      <td>0.5550</td>\n",
              "      <td>0.6711</td>\n",
              "      <td>0.6415</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6791</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>0.1307</td>\n",
              "      <td>0.2604</td>\n",
              "      <td>0.5121</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.6692</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.2834</td>\n",
              "      <td>0.2825</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.2641</td>\n",
              "      <td>0.1386</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       V1      V2      V3      V4  ...     V58     V59     V60  Class\n",
              "0  0.0200  0.0371  0.0428  0.0207  ...  0.0084  0.0090  0.0032      1\n",
              "1  0.0453  0.0523  0.0843  0.0689  ...  0.0049  0.0052  0.0044      1\n",
              "2  0.0262  0.0582  0.1099  0.1083  ...  0.0164  0.0095  0.0078      1\n",
              "3  0.0100  0.0171  0.0623  0.0205  ...  0.0044  0.0040  0.0117      1\n",
              "4  0.0762  0.0666  0.0481  0.0394  ...  0.0048  0.0107  0.0094      1\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZzhYnakUlfs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25a4c133-ba06-44c5-84af-fcca5b409171"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 208 entries, 0 to 207\n",
            "Data columns (total 61 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   V1      208 non-null    float64\n",
            " 1   V2      208 non-null    float64\n",
            " 2   V3      208 non-null    float64\n",
            " 3   V4      208 non-null    float64\n",
            " 4   V5      208 non-null    float64\n",
            " 5   V6      208 non-null    float64\n",
            " 6   V7      208 non-null    float64\n",
            " 7   V8      208 non-null    float64\n",
            " 8   V9      208 non-null    float64\n",
            " 9   V10     208 non-null    float64\n",
            " 10  V11     208 non-null    float64\n",
            " 11  V12     208 non-null    float64\n",
            " 12  V13     208 non-null    float64\n",
            " 13  V14     208 non-null    float64\n",
            " 14  V15     208 non-null    float64\n",
            " 15  V16     208 non-null    float64\n",
            " 16  V17     208 non-null    float64\n",
            " 17  V18     208 non-null    float64\n",
            " 18  V19     208 non-null    float64\n",
            " 19  V20     208 non-null    float64\n",
            " 20  V21     208 non-null    float64\n",
            " 21  V22     208 non-null    float64\n",
            " 22  V23     208 non-null    float64\n",
            " 23  V24     208 non-null    float64\n",
            " 24  V25     208 non-null    float64\n",
            " 25  V26     208 non-null    float64\n",
            " 26  V27     208 non-null    float64\n",
            " 27  V28     208 non-null    float64\n",
            " 28  V29     208 non-null    float64\n",
            " 29  V30     208 non-null    float64\n",
            " 30  V31     208 non-null    float64\n",
            " 31  V32     208 non-null    float64\n",
            " 32  V33     208 non-null    float64\n",
            " 33  V34     208 non-null    float64\n",
            " 34  V35     208 non-null    float64\n",
            " 35  V36     208 non-null    float64\n",
            " 36  V37     208 non-null    float64\n",
            " 37  V38     208 non-null    float64\n",
            " 38  V39     208 non-null    float64\n",
            " 39  V40     208 non-null    float64\n",
            " 40  V41     208 non-null    float64\n",
            " 41  V42     208 non-null    float64\n",
            " 42  V43     208 non-null    float64\n",
            " 43  V44     208 non-null    float64\n",
            " 44  V45     208 non-null    float64\n",
            " 45  V46     208 non-null    float64\n",
            " 46  V47     208 non-null    float64\n",
            " 47  V48     208 non-null    float64\n",
            " 48  V49     208 non-null    float64\n",
            " 49  V50     208 non-null    float64\n",
            " 50  V51     208 non-null    float64\n",
            " 51  V52     208 non-null    float64\n",
            " 52  V53     208 non-null    float64\n",
            " 53  V54     208 non-null    float64\n",
            " 54  V55     208 non-null    float64\n",
            " 55  V56     208 non-null    float64\n",
            " 56  V57     208 non-null    float64\n",
            " 57  V58     208 non-null    float64\n",
            " 58  V59     208 non-null    float64\n",
            " 59  V60     208 non-null    float64\n",
            " 60  Class   208 non-null    int64  \n",
            "dtypes: float64(60), int64(1)\n",
            "memory usage: 99.2 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhG8VEWmUom3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "eddb93d7-8628-4053-fa8d-a6fb5c3a1dc9"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>V29</th>\n",
              "      <th>V30</th>\n",
              "      <th>V31</th>\n",
              "      <th>V32</th>\n",
              "      <th>V33</th>\n",
              "      <th>V34</th>\n",
              "      <th>V35</th>\n",
              "      <th>V36</th>\n",
              "      <th>V37</th>\n",
              "      <th>V38</th>\n",
              "      <th>V39</th>\n",
              "      <th>V40</th>\n",
              "      <th>V41</th>\n",
              "      <th>V42</th>\n",
              "      <th>V43</th>\n",
              "      <th>V44</th>\n",
              "      <th>V45</th>\n",
              "      <th>V46</th>\n",
              "      <th>V47</th>\n",
              "      <th>V48</th>\n",
              "      <th>V49</th>\n",
              "      <th>V50</th>\n",
              "      <th>V51</th>\n",
              "      <th>V52</th>\n",
              "      <th>V53</th>\n",
              "      <th>V54</th>\n",
              "      <th>V55</th>\n",
              "      <th>V56</th>\n",
              "      <th>V57</th>\n",
              "      <th>V58</th>\n",
              "      <th>V59</th>\n",
              "      <th>V60</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "      <td>208.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.029164</td>\n",
              "      <td>0.038437</td>\n",
              "      <td>0.043832</td>\n",
              "      <td>0.053892</td>\n",
              "      <td>0.075202</td>\n",
              "      <td>0.104570</td>\n",
              "      <td>0.121747</td>\n",
              "      <td>0.134799</td>\n",
              "      <td>0.178003</td>\n",
              "      <td>0.208259</td>\n",
              "      <td>0.236013</td>\n",
              "      <td>0.250221</td>\n",
              "      <td>0.273305</td>\n",
              "      <td>0.296568</td>\n",
              "      <td>0.320201</td>\n",
              "      <td>0.378487</td>\n",
              "      <td>0.415983</td>\n",
              "      <td>0.452318</td>\n",
              "      <td>0.504812</td>\n",
              "      <td>0.563047</td>\n",
              "      <td>0.609060</td>\n",
              "      <td>0.624275</td>\n",
              "      <td>0.646975</td>\n",
              "      <td>0.672654</td>\n",
              "      <td>0.675424</td>\n",
              "      <td>0.699866</td>\n",
              "      <td>0.702155</td>\n",
              "      <td>0.694024</td>\n",
              "      <td>0.642074</td>\n",
              "      <td>0.580928</td>\n",
              "      <td>0.504475</td>\n",
              "      <td>0.439040</td>\n",
              "      <td>0.417220</td>\n",
              "      <td>0.403233</td>\n",
              "      <td>0.392571</td>\n",
              "      <td>0.384848</td>\n",
              "      <td>0.363807</td>\n",
              "      <td>0.339657</td>\n",
              "      <td>0.325800</td>\n",
              "      <td>0.311207</td>\n",
              "      <td>0.289252</td>\n",
              "      <td>0.278293</td>\n",
              "      <td>0.246542</td>\n",
              "      <td>0.214075</td>\n",
              "      <td>0.197232</td>\n",
              "      <td>0.160631</td>\n",
              "      <td>0.122453</td>\n",
              "      <td>0.091424</td>\n",
              "      <td>0.051929</td>\n",
              "      <td>0.020424</td>\n",
              "      <td>0.016069</td>\n",
              "      <td>0.013420</td>\n",
              "      <td>0.010709</td>\n",
              "      <td>0.010941</td>\n",
              "      <td>0.009290</td>\n",
              "      <td>0.008222</td>\n",
              "      <td>0.007820</td>\n",
              "      <td>0.007949</td>\n",
              "      <td>0.007941</td>\n",
              "      <td>0.006507</td>\n",
              "      <td>0.466346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.022991</td>\n",
              "      <td>0.032960</td>\n",
              "      <td>0.038428</td>\n",
              "      <td>0.046528</td>\n",
              "      <td>0.055552</td>\n",
              "      <td>0.059105</td>\n",
              "      <td>0.061788</td>\n",
              "      <td>0.085152</td>\n",
              "      <td>0.118387</td>\n",
              "      <td>0.134416</td>\n",
              "      <td>0.132705</td>\n",
              "      <td>0.140072</td>\n",
              "      <td>0.140962</td>\n",
              "      <td>0.164474</td>\n",
              "      <td>0.205427</td>\n",
              "      <td>0.232650</td>\n",
              "      <td>0.263677</td>\n",
              "      <td>0.261529</td>\n",
              "      <td>0.257988</td>\n",
              "      <td>0.262653</td>\n",
              "      <td>0.257818</td>\n",
              "      <td>0.255883</td>\n",
              "      <td>0.250175</td>\n",
              "      <td>0.239116</td>\n",
              "      <td>0.244926</td>\n",
              "      <td>0.237228</td>\n",
              "      <td>0.245657</td>\n",
              "      <td>0.237189</td>\n",
              "      <td>0.240250</td>\n",
              "      <td>0.220749</td>\n",
              "      <td>0.213992</td>\n",
              "      <td>0.213237</td>\n",
              "      <td>0.206513</td>\n",
              "      <td>0.231242</td>\n",
              "      <td>0.259132</td>\n",
              "      <td>0.264121</td>\n",
              "      <td>0.239912</td>\n",
              "      <td>0.212973</td>\n",
              "      <td>0.199075</td>\n",
              "      <td>0.178662</td>\n",
              "      <td>0.171111</td>\n",
              "      <td>0.168728</td>\n",
              "      <td>0.138993</td>\n",
              "      <td>0.133291</td>\n",
              "      <td>0.151628</td>\n",
              "      <td>0.133938</td>\n",
              "      <td>0.086953</td>\n",
              "      <td>0.062417</td>\n",
              "      <td>0.035954</td>\n",
              "      <td>0.013665</td>\n",
              "      <td>0.012008</td>\n",
              "      <td>0.009634</td>\n",
              "      <td>0.007060</td>\n",
              "      <td>0.007301</td>\n",
              "      <td>0.007088</td>\n",
              "      <td>0.005736</td>\n",
              "      <td>0.005785</td>\n",
              "      <td>0.006470</td>\n",
              "      <td>0.006181</td>\n",
              "      <td>0.005031</td>\n",
              "      <td>0.500070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>0.010200</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>0.007500</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.028900</td>\n",
              "      <td>0.023600</td>\n",
              "      <td>0.018400</td>\n",
              "      <td>0.027300</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.016200</td>\n",
              "      <td>0.034900</td>\n",
              "      <td>0.037500</td>\n",
              "      <td>0.049400</td>\n",
              "      <td>0.065600</td>\n",
              "      <td>0.051200</td>\n",
              "      <td>0.021900</td>\n",
              "      <td>0.056300</td>\n",
              "      <td>0.023900</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>0.092100</td>\n",
              "      <td>0.048100</td>\n",
              "      <td>0.028400</td>\n",
              "      <td>0.014400</td>\n",
              "      <td>0.061300</td>\n",
              "      <td>0.048200</td>\n",
              "      <td>0.040400</td>\n",
              "      <td>0.047700</td>\n",
              "      <td>0.021200</td>\n",
              "      <td>0.022300</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0.035100</td>\n",
              "      <td>0.038300</td>\n",
              "      <td>0.037100</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>0.005600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.013350</td>\n",
              "      <td>0.016450</td>\n",
              "      <td>0.018950</td>\n",
              "      <td>0.024375</td>\n",
              "      <td>0.038050</td>\n",
              "      <td>0.067025</td>\n",
              "      <td>0.080900</td>\n",
              "      <td>0.080425</td>\n",
              "      <td>0.097025</td>\n",
              "      <td>0.111275</td>\n",
              "      <td>0.129250</td>\n",
              "      <td>0.133475</td>\n",
              "      <td>0.166125</td>\n",
              "      <td>0.175175</td>\n",
              "      <td>0.164625</td>\n",
              "      <td>0.196300</td>\n",
              "      <td>0.205850</td>\n",
              "      <td>0.242075</td>\n",
              "      <td>0.299075</td>\n",
              "      <td>0.350625</td>\n",
              "      <td>0.399725</td>\n",
              "      <td>0.406925</td>\n",
              "      <td>0.450225</td>\n",
              "      <td>0.540725</td>\n",
              "      <td>0.525800</td>\n",
              "      <td>0.544175</td>\n",
              "      <td>0.531900</td>\n",
              "      <td>0.534775</td>\n",
              "      <td>0.463700</td>\n",
              "      <td>0.411400</td>\n",
              "      <td>0.345550</td>\n",
              "      <td>0.281400</td>\n",
              "      <td>0.257875</td>\n",
              "      <td>0.217575</td>\n",
              "      <td>0.179375</td>\n",
              "      <td>0.154350</td>\n",
              "      <td>0.160100</td>\n",
              "      <td>0.174275</td>\n",
              "      <td>0.173975</td>\n",
              "      <td>0.186450</td>\n",
              "      <td>0.163100</td>\n",
              "      <td>0.158900</td>\n",
              "      <td>0.155200</td>\n",
              "      <td>0.126875</td>\n",
              "      <td>0.094475</td>\n",
              "      <td>0.068550</td>\n",
              "      <td>0.064250</td>\n",
              "      <td>0.045125</td>\n",
              "      <td>0.026350</td>\n",
              "      <td>0.011550</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>0.007275</td>\n",
              "      <td>0.005075</td>\n",
              "      <td>0.005375</td>\n",
              "      <td>0.004150</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>0.003675</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.022800</td>\n",
              "      <td>0.030800</td>\n",
              "      <td>0.034300</td>\n",
              "      <td>0.044050</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.092150</td>\n",
              "      <td>0.106950</td>\n",
              "      <td>0.112100</td>\n",
              "      <td>0.152250</td>\n",
              "      <td>0.182400</td>\n",
              "      <td>0.224800</td>\n",
              "      <td>0.249050</td>\n",
              "      <td>0.263950</td>\n",
              "      <td>0.281100</td>\n",
              "      <td>0.281700</td>\n",
              "      <td>0.304700</td>\n",
              "      <td>0.308400</td>\n",
              "      <td>0.368300</td>\n",
              "      <td>0.434950</td>\n",
              "      <td>0.542500</td>\n",
              "      <td>0.617700</td>\n",
              "      <td>0.664900</td>\n",
              "      <td>0.699700</td>\n",
              "      <td>0.698500</td>\n",
              "      <td>0.721100</td>\n",
              "      <td>0.754500</td>\n",
              "      <td>0.745600</td>\n",
              "      <td>0.731900</td>\n",
              "      <td>0.680800</td>\n",
              "      <td>0.607150</td>\n",
              "      <td>0.490350</td>\n",
              "      <td>0.429600</td>\n",
              "      <td>0.391200</td>\n",
              "      <td>0.351050</td>\n",
              "      <td>0.312750</td>\n",
              "      <td>0.321150</td>\n",
              "      <td>0.306300</td>\n",
              "      <td>0.312700</td>\n",
              "      <td>0.283500</td>\n",
              "      <td>0.278050</td>\n",
              "      <td>0.259500</td>\n",
              "      <td>0.245100</td>\n",
              "      <td>0.222550</td>\n",
              "      <td>0.177700</td>\n",
              "      <td>0.148000</td>\n",
              "      <td>0.121350</td>\n",
              "      <td>0.101650</td>\n",
              "      <td>0.078100</td>\n",
              "      <td>0.044700</td>\n",
              "      <td>0.017900</td>\n",
              "      <td>0.013900</td>\n",
              "      <td>0.011400</td>\n",
              "      <td>0.009550</td>\n",
              "      <td>0.009300</td>\n",
              "      <td>0.007500</td>\n",
              "      <td>0.006850</td>\n",
              "      <td>0.005950</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.035550</td>\n",
              "      <td>0.047950</td>\n",
              "      <td>0.057950</td>\n",
              "      <td>0.064500</td>\n",
              "      <td>0.100275</td>\n",
              "      <td>0.134125</td>\n",
              "      <td>0.154000</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>0.233425</td>\n",
              "      <td>0.268700</td>\n",
              "      <td>0.301650</td>\n",
              "      <td>0.331250</td>\n",
              "      <td>0.351250</td>\n",
              "      <td>0.386175</td>\n",
              "      <td>0.452925</td>\n",
              "      <td>0.535725</td>\n",
              "      <td>0.659425</td>\n",
              "      <td>0.679050</td>\n",
              "      <td>0.731400</td>\n",
              "      <td>0.809325</td>\n",
              "      <td>0.816975</td>\n",
              "      <td>0.831975</td>\n",
              "      <td>0.848575</td>\n",
              "      <td>0.872175</td>\n",
              "      <td>0.873725</td>\n",
              "      <td>0.893800</td>\n",
              "      <td>0.917100</td>\n",
              "      <td>0.900275</td>\n",
              "      <td>0.852125</td>\n",
              "      <td>0.735175</td>\n",
              "      <td>0.641950</td>\n",
              "      <td>0.580300</td>\n",
              "      <td>0.556125</td>\n",
              "      <td>0.596125</td>\n",
              "      <td>0.593350</td>\n",
              "      <td>0.556525</td>\n",
              "      <td>0.518900</td>\n",
              "      <td>0.440550</td>\n",
              "      <td>0.434900</td>\n",
              "      <td>0.424350</td>\n",
              "      <td>0.387525</td>\n",
              "      <td>0.384250</td>\n",
              "      <td>0.324525</td>\n",
              "      <td>0.271750</td>\n",
              "      <td>0.231550</td>\n",
              "      <td>0.200375</td>\n",
              "      <td>0.154425</td>\n",
              "      <td>0.120100</td>\n",
              "      <td>0.068525</td>\n",
              "      <td>0.025275</td>\n",
              "      <td>0.020825</td>\n",
              "      <td>0.016725</td>\n",
              "      <td>0.014900</td>\n",
              "      <td>0.014500</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.010575</td>\n",
              "      <td>0.010425</td>\n",
              "      <td>0.010350</td>\n",
              "      <td>0.010325</td>\n",
              "      <td>0.008525</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.137100</td>\n",
              "      <td>0.233900</td>\n",
              "      <td>0.305900</td>\n",
              "      <td>0.426400</td>\n",
              "      <td>0.401000</td>\n",
              "      <td>0.382300</td>\n",
              "      <td>0.372900</td>\n",
              "      <td>0.459000</td>\n",
              "      <td>0.682800</td>\n",
              "      <td>0.710600</td>\n",
              "      <td>0.734200</td>\n",
              "      <td>0.706000</td>\n",
              "      <td>0.713100</td>\n",
              "      <td>0.997000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.998800</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.965700</td>\n",
              "      <td>0.930600</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.964700</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.949700</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.985700</td>\n",
              "      <td>0.929700</td>\n",
              "      <td>0.899500</td>\n",
              "      <td>0.824600</td>\n",
              "      <td>0.773300</td>\n",
              "      <td>0.776200</td>\n",
              "      <td>0.703400</td>\n",
              "      <td>0.729200</td>\n",
              "      <td>0.552200</td>\n",
              "      <td>0.333900</td>\n",
              "      <td>0.198100</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.100400</td>\n",
              "      <td>0.070900</td>\n",
              "      <td>0.039000</td>\n",
              "      <td>0.035200</td>\n",
              "      <td>0.044700</td>\n",
              "      <td>0.039400</td>\n",
              "      <td>0.035500</td>\n",
              "      <td>0.044000</td>\n",
              "      <td>0.036400</td>\n",
              "      <td>0.043900</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               V1          V2          V3  ...         V59         V60       Class\n",
              "count  208.000000  208.000000  208.000000  ...  208.000000  208.000000  208.000000\n",
              "mean     0.029164    0.038437    0.043832  ...    0.007941    0.006507    0.466346\n",
              "std      0.022991    0.032960    0.038428  ...    0.006181    0.005031    0.500070\n",
              "min      0.001500    0.000600    0.001500  ...    0.000100    0.000600    0.000000\n",
              "25%      0.013350    0.016450    0.018950  ...    0.003675    0.003100    0.000000\n",
              "50%      0.022800    0.030800    0.034300  ...    0.006400    0.005300    0.000000\n",
              "75%      0.035550    0.047950    0.057950  ...    0.010325    0.008525    1.000000\n",
              "max      0.137100    0.233900    0.305900  ...    0.036400    0.043900    1.000000\n",
              "\n",
              "[8 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpVHrmSEUtO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f0d995d4-d7b5-4a0c-e0f2-06fbe8714bc2"
      },
      "source": [
        "dataset['Class'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    111\n",
              "1     97\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeSKuJOCUz_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "73fc26b1-ba53-46e8-ce49-93656ab5c8b2"
      },
      "source": [
        "dataset.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
              "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
              "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31',\n",
              "       'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41',\n",
              "       'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51',\n",
              "       'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'Class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMw88VrOU4dC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27509ff8-4c51-43a3-aff7-00fc2ac72f66"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 208 entries, 0 to 207\n",
            "Data columns (total 61 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   V1      208 non-null    float64\n",
            " 1   V2      208 non-null    float64\n",
            " 2   V3      208 non-null    float64\n",
            " 3   V4      208 non-null    float64\n",
            " 4   V5      208 non-null    float64\n",
            " 5   V6      208 non-null    float64\n",
            " 6   V7      208 non-null    float64\n",
            " 7   V8      208 non-null    float64\n",
            " 8   V9      208 non-null    float64\n",
            " 9   V10     208 non-null    float64\n",
            " 10  V11     208 non-null    float64\n",
            " 11  V12     208 non-null    float64\n",
            " 12  V13     208 non-null    float64\n",
            " 13  V14     208 non-null    float64\n",
            " 14  V15     208 non-null    float64\n",
            " 15  V16     208 non-null    float64\n",
            " 16  V17     208 non-null    float64\n",
            " 17  V18     208 non-null    float64\n",
            " 18  V19     208 non-null    float64\n",
            " 19  V20     208 non-null    float64\n",
            " 20  V21     208 non-null    float64\n",
            " 21  V22     208 non-null    float64\n",
            " 22  V23     208 non-null    float64\n",
            " 23  V24     208 non-null    float64\n",
            " 24  V25     208 non-null    float64\n",
            " 25  V26     208 non-null    float64\n",
            " 26  V27     208 non-null    float64\n",
            " 27  V28     208 non-null    float64\n",
            " 28  V29     208 non-null    float64\n",
            " 29  V30     208 non-null    float64\n",
            " 30  V31     208 non-null    float64\n",
            " 31  V32     208 non-null    float64\n",
            " 32  V33     208 non-null    float64\n",
            " 33  V34     208 non-null    float64\n",
            " 34  V35     208 non-null    float64\n",
            " 35  V36     208 non-null    float64\n",
            " 36  V37     208 non-null    float64\n",
            " 37  V38     208 non-null    float64\n",
            " 38  V39     208 non-null    float64\n",
            " 39  V40     208 non-null    float64\n",
            " 40  V41     208 non-null    float64\n",
            " 41  V42     208 non-null    float64\n",
            " 42  V43     208 non-null    float64\n",
            " 43  V44     208 non-null    float64\n",
            " 44  V45     208 non-null    float64\n",
            " 45  V46     208 non-null    float64\n",
            " 46  V47     208 non-null    float64\n",
            " 47  V48     208 non-null    float64\n",
            " 48  V49     208 non-null    float64\n",
            " 49  V50     208 non-null    float64\n",
            " 50  V51     208 non-null    float64\n",
            " 51  V52     208 non-null    float64\n",
            " 52  V53     208 non-null    float64\n",
            " 53  V54     208 non-null    float64\n",
            " 54  V55     208 non-null    float64\n",
            " 55  V56     208 non-null    float64\n",
            " 56  V57     208 non-null    float64\n",
            " 57  V58     208 non-null    float64\n",
            " 58  V59     208 non-null    float64\n",
            " 59  V60     208 non-null    float64\n",
            " 60  Class   208 non-null    int64  \n",
            "dtypes: float64(60), int64(1)\n",
            "memory usage: 99.2 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVPr2xQhU_-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = dataset['Class'] # Target prediksi\n",
        "x = dataset.drop(['Class'], axis=1) # Data Id tidak diperlukan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OiYVEutVEpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(x)\n",
        "data_x = pd.DataFrame(X, columns=x.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGCz0-DXVFoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data_x, Y, test_size=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjhf-bN8VKt0",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6kIgeQPVHz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8e5b2736-22d2-4052-b1c7-5fcd5e982abf"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(x_train, y_train)\n",
        "y_pred = logreg.predict(x_test)\n",
        "print('Accuracy of logistic regression classifier on train set: {:.2f}'.format(logreg.score(x_train, y_train))) # .score() -> Accuracy\n",
        "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test))) # .score() -> Accuracy\n",
        "print(\"f1 score         :\",f1_score(y_test, y_pred, average='macro'))\n",
        "print(\"precision score  :\",precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"recall score     :\",recall_score(y_test, y_pred, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of logistic regression classifier on train set: 0.94\n",
            "Accuracy of logistic regression classifier on test set: 0.66\n",
            "f1 score         : 0.6559139784946236\n",
            "precision score  : 0.6607843137254902\n",
            "recall score     : 0.6626984126984128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHAzboWSVUQ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "da93b815-3077-4a53-eef6-bd6b173340f5"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.61      0.67        18\n",
            "           1       0.59      0.71      0.65        14\n",
            "\n",
            "    accuracy                           0.66        32\n",
            "   macro avg       0.66      0.66      0.66        32\n",
            "weighted avg       0.67      0.66      0.66        32\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYLPZW3gVVIV",
        "colab_type": "text"
      },
      "source": [
        "**Decision Tree Classifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmcMMMGNVcbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4d837f40-8217-463b-84ca-bcebe3d41d1d"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dectree = DecisionTreeClassifier()\n",
        "dectree.fit(x_train, y_train)\n",
        "y_pred = dectree.predict(x_test)\n",
        "print('Accuracy of Decision Tree Classifier on train set: {:.2f}'.format(dectree.score(x_train, y_train))) # .score() -> Accuracy\n",
        "print('Accuracy of Decision Tree Classifier on test set: {:.2f}'.format(dectree.score(x_test, y_test))) # .score() -> Accuracy\n",
        "print(\"f1 score         :\",f1_score(y_test, y_pred, average='macro'))\n",
        "print(\"precision score  :\",precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"recall score     :\",recall_score(y_test, y_pred, average='macro'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Decision Tree Classifier on train set: 1.00\n",
            "Accuracy of Decision Tree Classifier on test set: 0.72\n",
            "f1 score         : 0.7184750733137829\n",
            "precision score  : 0.723529411764706\n",
            "recall score     : 0.7261904761904762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y901r1eCVfyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2d04c4d9-7358-4647-a29e-5d6fe0652cdc"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.67      0.73        18\n",
            "           1       0.65      0.79      0.71        14\n",
            "\n",
            "    accuracy                           0.72        32\n",
            "   macro avg       0.72      0.73      0.72        32\n",
            "weighted avg       0.73      0.72      0.72        32\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcNt7hAaVjf1",
        "colab_type": "text"
      },
      "source": [
        "**SVM**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjeeEPY_Vmkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "07e057e5-3fed-4206-efbf-61ff47d4d826"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "def kernel_svc(ks):\n",
        "  classifier_svc = SVC(kernel = ks)\n",
        "  classifier_svc.fit(x_train, y_train)\n",
        "  y_pred = classifier_svc.predict(x_test)\n",
        "  print('Accuracy of SVC ',ks,' classifier on test set: {:.2f}'.format(classifier_svc.score(x_train, y_train))) # .score() -> Accuracy\n",
        "  print('Accuracy of SVC ',ks,' classifier on test set: {:.2f}'.format(classifier_svc.score(x_test, y_test)))\n",
        "  print(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "  print(precision_score(y_test, y_pred, average=\"macro\"))\n",
        "  print(recall_score(y_test, y_pred, average=\"macro\"))\n",
        "\n",
        "kernelsvc=['rbf','poly','linear']\n",
        "for x in kernelsvc:\n",
        "  print(\"SVC linear: \", x)\n",
        "  kernel_svc(x)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC linear:  rbf\n",
            "Accuracy of SVC  rbf  classifier on test set: 0.98\n",
            "Accuracy of SVC  rbf  classifier on test set: 0.78\n",
            "0.7793103448275862\n",
            "0.7784313725490195\n",
            "0.7817460317460317\n",
            "\n",
            "SVC linear:  poly\n",
            "Accuracy of SVC  poly  classifier on test set: 0.99\n",
            "Accuracy of SVC  poly  classifier on test set: 0.78\n",
            "0.7757757757757758\n",
            "0.7793522267206479\n",
            "0.7738095238095238\n",
            "\n",
            "SVC linear:  linear\n",
            "Accuracy of SVC  linear  classifier on test set: 0.97\n",
            "Accuracy of SVC  linear  classifier on test set: 0.72\n",
            "0.7162561576354679\n",
            "0.7156862745098038\n",
            "0.7182539682539683\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLqhkIFMVsav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2d1d39a7-004c-47a1-fb06-074c41013344"
      },
      "source": [
        "classifier_linear_svc = SVC()\n",
        "classifier_linear_svc.fit(x_train, y_train)\n",
        "y_pred = classifier_linear_svc.predict(x_test)\n",
        "print('Accuracy of LinearSVC classifier on train set: {:.2f}'.format(classifier_linear_svc.score(x_train, y_train))) # .score() -> Accuracy\n",
        "print('Accuracy of LinearSVC classifier on test set: {:.2f}'.format(classifier_linear_svc.score(x_test, y_test)))\n",
        "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
        "print(recall_score(y_test, y_pred, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of LinearSVC classifier on train set: 0.98\n",
            "Accuracy of LinearSVC classifier on test set: 0.78\n",
            "0.7793103448275862\n",
            "0.7784313725490195\n",
            "0.7817460317460317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZs1nmLaVpQn",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkyn3HQ2Vyec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f4c60d63-f6da-4bb8-ae40-f8fb174a6d9a"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier(n_estimators=300, bootstrap=True, max_features = \"sqrt\")\n",
        "rf_model.fit(x_train, y_train)\n",
        "y_pred = rf_model.predict(x_test)\n",
        "print('Accuracy of Random Forest on train set: {:.2f}'.format(rf_model.score(x_train, y_train)))\n",
        "print('Accuracy of Random Forest on test set: {:.2f}'.format(rf_model.score(x_test, y_test)))\n",
        "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
        "print(recall_score(y_test, y_pred, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Random Forest on train set: 1.00\n",
            "Accuracy of Random Forest on test set: 0.84\n",
            "0.8358974358974359\n",
            "0.8593073593073592\n",
            "0.8293650793650793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9EQ7sYpV0Z1",
        "colab_type": "text"
      },
      "source": [
        "**K-Nearest Neighbors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9elVwNoV3qa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "7889eb50-036b-41de-be54-939922022c45"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "def k_near(knnr):\n",
        "  # knear = KNeighborsClassifier(knnr)\n",
        "  # knear.fit(x_train, y_train)\n",
        "  # y_pred = knear.predict(x_test)\n",
        "  # print('Accuracy of KNN on train set: {:.2f}'.format(knear.score(x_train, y_train)))\n",
        "  # print('Accuracy of KNN on test set: {:.2f}'.format(knear.score(x_test, y_test)))\n",
        "  # print(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "  # print(precision_score(y_test, y_pred, average=\"macro\"))\n",
        "  # print(recall_score(y_test, y_pred, average=\"macro\"))\n",
        "  \n",
        "  knn = KNeighborsClassifier(knnr)\n",
        "  knn.fit(x_train, y_train)\n",
        "  f1_score(knn.predict(x_train), y_train)\n",
        "  cross_validation_score = cross_val_score(knn, x_train, y_train, cv=10)\n",
        "  knn_score = f1_score(knn.predict(x_test), y_test) \n",
        "  return knn_score\n",
        "\n",
        "# k=[1,3,5,7,9,11,13,15,99]\n",
        "# for x in k:\n",
        "#   print(\"K Nearest Neighbor: \", x)\n",
        "#   k_near(x)\n",
        "#   print()\n",
        "\n",
        "k=[1,3,5,7,9,11,13,15,99]\n",
        "for x in k:\n",
        "  print(\"K Nearest Neighbor: \", x)\n",
        "  print(k_near(x))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K Nearest Neighbor:  1\n",
            "0.9285714285714286\n",
            "\n",
            "K Nearest Neighbor:  3\n",
            "0.8461538461538461\n",
            "\n",
            "K Nearest Neighbor:  5\n",
            "0.7407407407407408\n",
            "\n",
            "K Nearest Neighbor:  7\n",
            "0.8275862068965518\n",
            "\n",
            "K Nearest Neighbor:  9\n",
            "0.7586206896551724\n",
            "\n",
            "K Nearest Neighbor:  11\n",
            "0.689655172413793\n",
            "\n",
            "K Nearest Neighbor:  13\n",
            "0.6428571428571429\n",
            "\n",
            "K Nearest Neighbor:  15\n",
            "0.689655172413793\n",
            "\n",
            "K Nearest Neighbor:  99\n",
            "0.5925925925925927\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhFK5EP7V7zZ",
        "colab_type": "text"
      },
      "source": [
        "**XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBXK5ku5V_UI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "fa567e48-3f55-4b24-ed9b-ac74113a3428"
      },
      "source": [
        "xgb_model = xgb.XGBClassifier(objective=\"reg:linear\")\n",
        "xgb_model.fit(x_train, y_train)\n",
        "y_pred = xgb_model.predict(x_test)\n",
        "print('Accuracy of XGBoost on train set: {:.2f}'.format(xgb_model.score(x_train, y_train)))\n",
        "print('Accuracy of XGBoost on test set: {:.2f}'.format(xgb_model.score(x_test, y_test)))\n",
        "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
        "print(recall_score(y_test, y_pred, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[09:28:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Accuracy of XGBoost on train set: 1.00\n",
            "Accuracy of XGBoost on test set: 0.75\n",
            "0.7490196078431373\n",
            "0.75\n",
            "0.753968253968254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4bG4ydaWENm",
        "colab_type": "text"
      },
      "source": [
        "**Gaussian Process Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPcv82wOWOQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "64267981-ec5e-4ff6-ad50-2a9b06d014dc"
      },
      "source": [
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "\n",
        "\n",
        "gaus_model = GaussianProcessClassifier()\n",
        "gaus_model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = gaus_model.predict(x_test)\n",
        "print('Accuracy of Gauss on train set: {:.2f}'.format(gaus_model.score(x_train, y_train)))\n",
        "print('Accuracy of Gauss on test set: {:.2f}'.format(gaus_model.score(x_test, y_test)))\n",
        "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
        "print(recall_score(y_test, y_pred, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Gauss on train set: 1.00\n",
            "Accuracy of Gauss on test set: 0.91\n",
            "0.903903903903904\n",
            "0.9089068825910931\n",
            "0.9007936507936507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnV5HLtSWSYt",
        "colab_type": "text"
      },
      "source": [
        "**MLP Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVsJijpoWUVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "130037a5-e231-4d22-86ee-3388c56de9e1"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "MLP_model = MLPClassifier(hidden_layer_sizes=(3,), activation='logistic',\n",
        "                       solver='adam', alpha=0.0001,learning_rate='constant', \n",
        "                      learning_rate_init=0.3)\n",
        "MLP_model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = MLP_model.predict(x_test)\n",
        "print('Accuracy of MLP on train set: {:.2f}'.format(MLP_model.score(x_train, y_train)))\n",
        "print('Accuracy of MLP on test set: {:.2f}'.format(MLP_model.score(x_test, y_test)))\n",
        "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
        "print(recall_score(y_test, y_pred, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of MLP on train set: 0.95\n",
            "Accuracy of MLP on test set: 0.69\n",
            "0.6875000000000001\n",
            "0.6984126984126984\n",
            "0.6984126984126984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPgIMMe7WX_L",
        "colab_type": "text"
      },
      "source": [
        "**ADA Boost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWmmpGRuWbFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bcdd4d5b-7568-439e-91f4-869839692461"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "ada_model = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "ada_model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = ada_model.predict(x_test)\n",
        "print('Accuracy of ADA Boost on train set: {:.2f}'.format(ada_model.score(x_train, y_train)))\n",
        "print('Accuracy of ADA Boost on test set: {:.2f}'.format(ada_model.score(x_test, y_test)))\n",
        "print(f1_score(y_test, y_pred, average=\"macro\"))\n",
        "print(precision_score(y_test, y_pred, average=\"macro\"))\n",
        "print(recall_score(y_test, y_pred, average=\"macro\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of ADA Boost on train set: 1.00\n",
            "Accuracy of ADA Boost on test set: 0.81\n",
            "0.8095238095238095\n",
            "0.8095238095238095\n",
            "0.8095238095238095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W4FvwhWNREK",
        "colab_type": "text"
      },
      "source": [
        "Dari algoritma algoritma yang saya gunakan, algoritma yang memiliki tingkat akurasi terbaik adalah gauss proccess classifier 91%"
      ]
    }
  ]
}